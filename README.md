# Treinamento Data Hands-On Modern Data Stack AWS

Este reposit√≥rio cont√©m o c√≥digo e a infraestrutura para um projeto de Modern Data Stack implementado na AWS, utilizando o dataset MovieLens para demonstrar diferentes t√©cnicas de processamento de dados.

## Estrutura do Projeto

### üìÅ code/
Cont√©m todos os c√≥digos de aplica√ß√£o para processamento de dados.

#### üìÅ dbt/
Implementa√ß√£o do dbt (data build tool) para transforma√ß√£o de dados no Redshift.

- **airflow_dags/**: DAGs do Airflow para orquestra√ß√£o do dbt
  - `dag_movielens_dbt_redshift_mwaa.py`: DAG para execu√ß√£o do dbt no MWAA (Amazon Managed Workflows for Apache Airflow)
  - `dag_movielens_dbt_redshift.py`: DAG para execu√ß√£o do dbt no Redshift
  - `requirements.txt`: Depend√™ncias necess√°rias
  - `startup-script-mwaa.sh`: Script de inicializa√ß√£o para o MWAA

- **movielens_redshift/**: Projeto dbt para transforma√ß√£o de dados do MovieLens
  - `models/`: Modelos dbt organizados em camadas
    - `staging/`: Modelos de staging para ingest√£o inicial
    - `intermediate/`: Modelos intermedi√°rios
    - `analytics/`: Modelos finais para an√°lise

#### üìÅ emr/
C√≥digo para processamento de dados usando Amazon EMR (Elastic MapReduce).

- **modules/**: M√≥dulos Python para processamento Spark
  - `jobs/`: Defini√ß√µes de jobs Spark
  - `utils/`: Utilit√°rios para logging e opera√ß√µes Spark
- `bootstrap.sh`: Script de bootstrap para clusters EMR
- `dag_emr_movielens.py`: DAG do Airflow para orquestra√ß√£o de jobs EMR
- `main.py`: Ponto de entrada principal

#### üìÅ glue-jobs/
Scripts AWS Glue para ETL de dados.

- `jars/`: JARs necess√°rios para os jobs Glue
- Scripts para processamento de dados em diferentes formatos:
  - Delta Lake: `datahandson-mds-*-deltalake-*.py`
  - Apache Iceberg: `datahandson-mds-s3tables-*-iceberg-*.py`

#### üìÅ insert-data-postgres/
Scripts para inser√ß√£o de dados no PostgreSQL.

- `script-python-insert-csv-postgres.py`: Script Python para inser√ß√£o de dados
- `script-spark-insert-csv-postgres.py`: Script Spark para inser√ß√£o de dados

#### üìÅ kinesis-firehose/
Scripts para envio de eventos para o Amazon Kinesis Firehose.

- `script-send-events-kinesis.py`: Script para envio de eventos

#### üìÅ lambda_code_ecr/
C√≥digo para fun√ß√µes Lambda usando cont√™ineres ECR.

- `Dockerfile`: Defini√ß√£o do cont√™iner
- `lambda_handler.py`: C√≥digo da fun√ß√£o Lambda
- `build_and_push.sh`: Script para build e push da imagem

#### üìÅ sfn-orchestration-glue-jobs/
Defini√ß√µes de fluxos de trabalho do AWS Step Functions para orquestra√ß√£o de jobs Glue.

- `datahandson-mds-movielens-glue-etl.json`: Defini√ß√£o para ETL do MovieLens
- `datahandson-mds-s3tables-movielens-glue-etl.json`: Defini√ß√£o para ETL usando S3 Tables

### üìÅ data/
Cont√©m os dados de entrada para o projeto.

- `ml-latest-small.zip`: Dataset MovieLens em formato compactado

### üìÅ terraform/
C√≥digo de infraestrutura como c√≥digo (IaC) usando Terraform.

#### üìÅ infra/
Defini√ß√µes de infraestrutura AWS.

- **backends/**: Configura√ß√µes de backend do Terraform
- **envs/**: Vari√°veis de ambiente para diferentes ambientes
- **modules/**: M√≥dulos Terraform reutiliz√°veis
  - `dms-serverless/`: AWS Database Migration Service Serverless
  - `ec2/`: Inst√¢ncias EC2
  - `glue-crawler-delta/`: Crawlers Glue para Delta Lake
  - `glue-job/`: Jobs AWS Glue
  - `kinesis-firehose/`: Kinesis Firehose
  - `kinesis-stream/`: Kinesis Data Streams
  - `lambda/`: Fun√ß√µes Lambda
  - `mwaa/`: Amazon Managed Workflows for Apache Airflow
  - `opensearch-serverless/`: Amazon OpenSearch Serverless
  - `rds/`: Amazon RDS (Relational Database Service)
  - `redshift/`: Amazon Redshift
  - `step-functions/`: AWS Step Functions
  - `vpc/`: Amazon VPC (Virtual Private Cloud)
- **scripts/**: Scripts auxiliares para a infraestrutura

## Fluxo de Dados

Este projeto implementa um pipeline de dados completo:

1. **Ingest√£o**: Dados s√£o ingeridos via Kinesis ou carregados diretamente do dataset MovieLens
2. **Armazenamento**: Os dados s√£o armazenados em diferentes formatos (Delta Lake, Iceberg) no S3
3. **Processamento**: Transforma√ß√µes s√£o realizadas usando AWS Glue, EMR e dbt
4. **An√°lise**: Os dados processados s√£o disponibilizados para an√°lise no Redshift
5. **Orquestra√ß√£o**: O fluxo √© orquestrado usando Airflow (MWAA) e Step Functions

## Tecnologias Utilizadas

- **Armazenamento**: Amazon S3, Delta Lake, Apache Iceberg
- **Processamento**: AWS Glue, Amazon EMR, Apache Spark
- **Data Warehouse**: Amazon Redshift
- **Banco de Dados**: Amazon RDS (PostgreSQL)
- **Streaming**: Amazon Kinesis Data Streams, Kinesis Firehose
- **Orquestra√ß√£o**: Amazon MWAA (Airflow), AWS Step Functions
- **Computa√ß√£o Serverless**: AWS Lambda
- **Busca e An√°lise**: Amazon OpenSearch Serverless
- **Infraestrutura como C√≥digo**: Terraform
- **Transforma√ß√£o de Dados**: dbt (data build tool)

## Como Come√ßar

1. Configure suas credenciais AWS
2. Navegue at√© o diret√≥rio `terraform/infra`
3. Execute:
   ```bash
   terraform init -backend-config=backends/develop.hcl
   terraform plan -var-file=envs/develop.tfvars
   terraform apply -var-file=envs/develop.tfvars
   ```
4. Ap√≥s a cria√ß√£o da infraestrutura, execute os pipelines de dados conforme necess√°rio